Excellent r√©flexe d‚Äôavoir pris la photo ‚Äî cette fois, l‚Äôerreur est diff√©rente et plus profonde c√¥t√© Spark shuffle.
Regardons la partie cl√© du log üëá

org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 1
at org.apache.spark.MapOutputTracker$.validateStatus(...)


---

üí• Cause technique

Cette erreur :

> MetadataFetchFailedException: Missing an output location for shuffle



signifie que Spark ne retrouve pas la sortie d‚Äôune t√¢che shuffle (stage pr√©c√©dent).
C‚Äôest un probl√®me typique de shuffle fetch failure, li√© √† une des causes suivantes :


---

‚ö†Ô∏è Causes fr√©quentes

1Ô∏è‚É£ Un ou plusieurs executors sont morts / supprim√©s avant la lecture du shuffle

‚û° Spark perd les m√©tadonn√©es d‚Äôemplacement (shuffle blocks).
Sympt√¥mes :

Erreur Missing an output location

T√¢ches relanc√©es plusieurs fois avant l‚Äôabandon

Peut √™tre intermittent


Solutions possibles :

Augmenter la tol√©rance :

--conf spark.task.maxFailures=8

V√©rifier les logs des executors (OOM, timeouts, pertes r√©seau)



---

2Ô∏è‚É£ Un disque temporaire / r√©pertoire shuffle supprim√© ou plein

‚û° Spark √©crit les fichiers shuffle sur /tmp ou /localdir (configurable).

Solutions :

V√©rifie les disques sur les n≈ìuds ex√©cutants :

df -h

Nettoyer /tmp si un nettoyage automatique supprime les blocs trop t√¥t.

Ajouter un r√©pertoire shuffle explicite :

--conf spark.local.dir=/mnt/spark/tmp



---

3Ô∏è‚É£ Cluster instable (AKS, YARN, K8s)

Si tu es sur AKS (Azure Kubernetes Service) (ce qui est ton cas), il arrive que :

un Pod Spark Executor soit tu√© par le scheduler Kubernetes (OOM, eviction, scale down‚Ä¶)

donc Spark perd les fichiers shuffle interm√©diaires (non r√©pliqu√©s).


Solutions AKS sp√©cifiques :

--conf spark.shuffle.service.enabled=true
--conf spark.dynamicAllocation.enabled=true
--conf spark.shuffle.service.port=7337
--conf spark.executor.memoryOverhead=2048

üí° Cela garde les fichiers shuffle m√™me si un executor est rel√¢ch√©.


---

4Ô∏è‚É£ Op√©rations lourdes avant le write (join, groupByKey, repartition‚Ä¶)

Si ton SparkExportHelper fait un join, groupByKey, distinct ou repartition avant le .write(), alors le shuffle peut saturer.

Solutions :

Utiliser .coalesce(n) avant le .write() pour r√©duire les partitions :

dataset.coalesce(10)
       .write()
       .mode(SaveMode.Overwrite)
       .parquet(path);

Ou utiliser repartition() uniquement si n√©cessaire, pas syst√©matiquement.



---

5Ô∏è‚É£ Timeout entre driver et executors

V√©rifie dans les logs s‚Äôil y a des lignes comme :

Lost executor X due to heartbeat timeout

ou

Connection reset by peer

Si oui, augmente :

--conf spark.network.timeout=600s
--conf spark.executor.heartbeatInterval=60s



---

‚úÖ R√©sum√© rapide des actions √† tester

Cat√©gorie	Action

Existence du r√©pertoire	.mode(SaveMode.Overwrite)
Nombre de partitions	.coalesce(10) avant .write()
Shuffle persistant	--conf spark.shuffle.service.enabled=true
Timeout r√©seau	--conf spark.network.timeout=600s
Cluster stable	V√©rifie les logs AKS pour PodKilled / OOM
Espace disque	df -h sur les n≈ìuds ex√©cutants



---

Souhaites-tu que je t‚Äôenvoie un exemple de configuration SparkSession AKS robuste (avec toutes les options r√©seau, shuffle, m√©moire et retry configur√©es pour √©viter ce type d‚Äôerreur) ?

