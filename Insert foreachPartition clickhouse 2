Parfait — voici une version 100 % sans Spring, réutilisable, paramétrable (table, colonnes, batch size, retry, timeouts), avec PreparedStatement (ps.set...) et gestion des NULL. Tu l’appelles depuis foreachPartition après ton coalesce(2).


---

1) Utilitaire générique ClickHouse JDBC (Java 17)

import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.*;

import java.math.BigDecimal;
import java.sql.*;
import java.time.ZoneId;
import java.util.*;
import java.util.concurrent.TimeUnit;

public class ClickHouseJdbcWriter {

    public static class Config {
        public String jdbcUrl;            // ex: "jdbc:clickhouse://127.0.0.1:9000/default"
        public String user = "default";
        public String password = "";
        public String table;              // "db.table" ou "table" si database par défaut
        public List<String> columns;      // ordre des colonnes d'INSERT
        public int batchSize = 50_000;    // lignes par executeBatch()
        public int maxRetries = 3;        // tentatives par partition
        public long baseBackoffMs = 2_000;
        public int connectTimeoutMs = 15_000;
        public int socketTimeoutMs = 60_000;
        public ZoneId tz = ZoneId.systemDefault(); // pour Timestamp si besoin

        public Config with(String key, Object val) { // petit helper fluide
            switch (key) {
                case "jdbcUrl" -> this.jdbcUrl = String.valueOf(val);
                case "user" -> this.user = String.valueOf(val);
                case "password" -> this.password = String.valueOf(val);
                case "table" -> this.table = String.valueOf(val);
                case "batchSize" -> this.batchSize = (int) val;
                case "maxRetries" -> this.maxRetries = (int) val;
                case "baseBackoffMs" -> this.baseBackoffMs = (long) val;
                case "connectTimeoutMs" -> this.connectTimeoutMs = (int) val;
                case "socketTimeoutMs" -> this.socketTimeoutMs = (int) val;
                case "tz" -> this.tz = (ZoneId) val;
                default -> {}
            }
            return this;
        }
    }

    private final Config cfg;

    public ClickHouseJdbcWriter(Config cfg) {
        Objects.requireNonNull(cfg.jdbcUrl, "jdbcUrl manquant");
        Objects.requireNonNull(cfg.table, "table manquante");
        Objects.requireNonNull(cfg.columns, "columns manquantes");
        if (cfg.columns.isEmpty()) throw new IllegalArgumentException("columns ne peut pas être vide");
        this.cfg = cfg;
    }

    public void writePartition(Iterator<Row> rows, StructType schema) throws Exception {
        int attempt = 0;
        while (true) {
            try (Connection conn = openConnection();
                 PreparedStatement ps = conn.prepareStatement(buildInsertSql(cfg.table, cfg.columns))) {

                conn.setAutoCommit(false);

                int buffered = 0;
                while (rows.hasNext()) {
                    Row r = rows.next();
                    bindRow(ps, r, schema, cfg.columns);
                    ps.addBatch();
                    buffered++;

                    if (buffered >= cfg.batchSize) {
                        ps.executeBatch();
                        conn.commit();
                        buffered = 0;
                    }
                }

                if (buffered > 0) {
                    ps.executeBatch();
                    conn.commit();
                }

                // succès
                return;

            } catch (SQLException ex) {
                attempt++;
                if (attempt >= cfg.maxRetries) {
                    throw new RuntimeException("Echec ClickHouse après " + cfg.maxRetries + " tentatives", ex);
                }
                long sleep = cfg.baseBackoffMs * attempt;
                System.err.println("[ClickHouseJdbcWriter] tentative " + attempt + " échouée: "
                        + ex.getMessage() + " — retry dans " + sleep + " ms");
                Thread.sleep(sleep);
            }
        }
    }

    private Connection openConnection() throws SQLException {
        // La plupart des drivers ClickHouse acceptent des propriétés de timeout
        Properties props = new Properties();
        props.setProperty("user", cfg.user);
        props.setProperty("password", cfg.password);
        props.setProperty("socket_timeout", String.valueOf(cfg.socketTimeoutMs)); // driver ClickHouse
        props.setProperty("connect_timeout", String.valueOf(cfg.connectTimeoutMs));
        // props.setProperty("compress", "true"); // optionnel
        return DriverManager.getConnection(cfg.jdbcUrl, props);
    }

    private static String buildInsertSql(String table, List<String> columns) {
        String cols = String.join(",", columns);
        String qs = String.join(",", Collections.nCopies(columns.size(), "?"));
        return "INSERT INTO " + table + " (" + cols + ") VALUES (" + qs + ")";
    }

    private void bindRow(PreparedStatement ps, Row r, StructType schema, List<String> columns) throws SQLException {
        // On mappe colonne par nom → index de schema
        for (int i = 0; i < columns.size(); i++) {
            String colName = columns.get(i);
            int schemaIndex = schema.fieldIndex(colName);
            StructField f = schema.fields()[schemaIndex];
            Object v = r.isNullAt(schemaIndex) ? null : r.get(schemaIndex);
            setParam(ps, i + 1, f.dataType(), v);
        }
    }

    private void setParam(PreparedStatement ps, int idx, DataType dt, Object v) throws SQLException {
        if (v == null) {
            ps.setNull(idx, sqlNullType(dt));
            return;
        }

        // mapping des types Spark → JDBC setX
        if (dt instanceof IntegerType) {
            ps.setInt(idx, ((Number) v).intValue());
        } else if (dt instanceof LongType) {
            ps.setLong(idx, ((Number) v).longValue());
        } else if (dt instanceof ShortType) {
            ps.setShort(idx, ((Number) v).shortValue());
        } else if (dt instanceof ByteType) {
            ps.setByte(idx, ((Number) v).byteValue());
        } else if (dt instanceof DoubleType) {
            ps.setDouble(idx, ((Number) v).doubleValue());
        } else if (dt instanceof FloatType) {
            ps.setFloat(idx, ((Number) v).floatValue());
        } else if (dt instanceof DecimalType) {
            ps.setBigDecimal(idx, (v instanceof BigDecimal) ? (BigDecimal) v : new BigDecimal(v.toString()));
        } else if (dt instanceof BooleanType) {
            ps.setBoolean(idx, (Boolean) v);
        } else if (dt instanceof StringType) {
            ps.setString(idx, String.valueOf(v));
        } else if (dt instanceof TimestampType) {
            // v peut être java.sql.Timestamp ou long (epoch ms) selon source
            if (v instanceof Timestamp ts) {
                ps.setTimestamp(idx, ts);
            } else if (v instanceof java.time.Instant ins) {
                ps.setTimestamp(idx, Timestamp.from(ins));
            } else if (v instanceof Long ms) {
                ps.setTimestamp(idx, new Timestamp(ms));
            } else {
                ps.setTimestamp(idx, Timestamp.valueOf(v.toString()));
            }
        } else if (dt instanceof DateType) {
            if (v instanceof java.sql.Date d) {
                ps.setDate(idx, d);
            } else if (v instanceof java.time.LocalDate ld) {
                ps.setDate(idx, java.sql.Date.valueOf(ld));
            } else if (v instanceof Long days) {
                ps.setDate(idx, new java.sql.Date(TimeUnit.DAYS.toMillis(days)));
            } else {
                ps.setDate(idx, java.sql.Date.valueOf(v.toString()));
            }
        } else if (dt instanceof BinaryType) {
            ps.setBytes(idx, (byte[]) v);
        } else {
            // fallback : stringify (à adapter si MapType/ArrayType/StructType → JSON)
            ps.setString(idx, v.toString());
        }
    }

    private int sqlNullType(DataType dt) {
        if (dt instanceof IntegerType) return Types.INTEGER;
        if (dt instanceof LongType) return Types.BIGINT;
        if (dt instanceof ShortType) return Types.SMALLINT;
        if (dt instanceof ByteType) return Types.TINYINT;
        if (dt instanceof DoubleType) return Types.DOUBLE;
        if (dt instanceof FloatType) return Types.FLOAT;
        if (dt instanceof DecimalType) return Types.DECIMAL;
        if (dt instanceof BooleanType) return Types.BOOLEAN;
        if (dt instanceof StringType) return Types.VARCHAR;
        if (dt instanceof TimestampType) return Types.TIMESTAMP;
        if (dt instanceof DateType) return Types.DATE;
        if (dt instanceof BinaryType) return Types.BINARY;
        return Types.VARCHAR;
    }
}


---

2) Utilisation côté Spark (avec coalesce(2))

// 1) prépa DataFrame
Dataset<Row> df = /* tes transformations */;

// 2) sélectionner exactement les colonnes dans l’ordre d’INSERT
List<String> cols = List.of("id", "name", "value");

// 3) limiter le parallélisme d’écriture
Dataset<Row> out = df.selectExpr(cols.toArray(new String[0])).coalesce(2);

// 4) écrire par partition avec retry
ClickHouseJdbcWriter.Config cfg = new ClickHouseJdbcWriter.Config();
cfg.jdbcUrl = "jdbc:clickhouse://127.0.0.1:9000/default";
cfg.user = "default";
cfg.password = "xxx";
cfg.table = "my_database.my_table"; // ou "my_table"
cfg.columns = cols;
cfg.batchSize = 100_000;   // ajuste selon la taille des lignes
cfg.maxRetries = 3;
cfg.connectTimeoutMs = 15_000;
cfg.socketTimeoutMs = 60_000;

out.foreachPartition(part -> {
    ClickHouseJdbcWriter writer = new ClickHouseJdbcWriter(cfg);
    writer.writePartition(part, out.schema());
});


---

Notes pratiques

Colonnes dynamiques : change juste cols et l’ordre du selectExpr.

Types spéciaux : pour Array/Map/Struct, convertis en JSON avant (to_json(col("..."))) et déclare la colonne en String côté ClickHouse (ou utilise des types CH natifs si tu veux aller plus loin).

Idempotence : si tu relances, préfère INSERT INTO ... sur des tables ReplacingMergeTree avec clé de dédup (ex: _version) ou fais un staging + swap.

Diagnostic CH :

SELECT table, count() parts FROM system.parts WHERE active GROUP BY table ORDER BY parts DESC;
SELECT query_id, rows_written, query_duration_ms FROM system.query_log
WHERE type='QueryFinish' AND query ILIKE 'insert%' ORDER BY event_time DESC LIMIT 20;



---

Si tu veux, je te fournis aussi une variante “générique JSON” (auto-sérialise Struct/Array en JSON) ou une version avec contrôle du INSERT en clusters (Distributed vs shard direct).
