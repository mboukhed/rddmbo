Parfait üëç
Voici la version compl√®te et finale de la classe PartitionDescriptor avec la m√©thode utilitaire fromRow(Row, List<String>), qui permet de convertir directement une ligne Spark (Row) en objet partition :


---

‚úÖ Classe Java finale : PartitionDescriptor

import org.apache.spark.sql.Row;

import java.io.Serializable;
import java.util.*;
import java.util.stream.Collectors;

/**
 * Repr√©sente une partition Spark sous forme de mapping colonne->valeur.
 * Exemple : { dt=2025-11-01, hour=13 }
 */
public class PartitionDescriptor implements Serializable {

    private final LinkedHashMap<String, String> values = new LinkedHashMap<>();

    /** Constructeur vide */
    public PartitionDescriptor() {}

    /** Constructeur √† partir d‚Äôune Map */
    public PartitionDescriptor(Map<String, ?> map) {
        if (map != null) {
            map.forEach((k, v) -> values.put(k, v == null ? null : v.toString()));
        }
    }

    /** Ajoute une cl√©/valeur */
    public PartitionDescriptor add(String column, Object value) {
        values.put(column, value == null ? null : value.toString());
        return this;
    }

    /** Retourne la valeur d'une colonne */
    public String get(String column) {
        return values.get(column);
    }

    /** Colonnes de partition (ordre d‚Äôinsertion) */
    public List<String> getColumns() {
        return new ArrayList<>(values.keySet());
    }

    /** Valeurs */
    public List<String> getValues() {
        return new ArrayList<>(values.values());
    }

    /** Formate en chemin Hive-style : col1=val1/col2=val2/ */
    public String toHivePath() {
        return values.entrySet().stream()
                .map(e -> e.getKey() + "=" + e.getValue())
                .collect(Collectors.joining("/", "", "/"));
    }

    /** Formate en pr√©dicat SQL : col1='val1' AND col2='val2' */
    public String toSqlPredicate() {
        return values.entrySet().stream()
                .map(e -> e.getKey() + "='" + e.getValue() + "'")
                .collect(Collectors.joining(" AND "));
    }

    /** Repr√©sentation lisible */
    @Override
    public String toString() {
        return values.toString();
    }

    /** Retourne la map interne (non modifiable) */
    public Map<String, String> asMap() {
        return Collections.unmodifiableMap(values);
    }

    // ------------------ M√©thodes statiques utilitaires ------------------

    /** Cr√©e une PartitionDescriptor √† partir d‚Äôun Row Spark et d‚Äôune liste de colonnes */
    public static PartitionDescriptor fromRow(Row row, List<String> partitionCols) {
        PartitionDescriptor pd = new PartitionDescriptor();
        for (String col : partitionCols) {
            Object val = row.getAs(col);
            pd.add(col, val);
        }
        return pd;
    }

    /** Fabriques rapides */
    public static PartitionDescriptor of(String k1, Object v1) {
        return new PartitionDescriptor().add(k1, v1);
    }

    public static PartitionDescriptor of(String k1, Object v1, String k2, Object v2) {
        return new PartitionDescriptor().add(k1, v1).add(k2, v2);
    }

    public static PartitionDescriptor of(String k1, Object v1, String k2, Object v2, String k3, Object v3) {
        return new PartitionDescriptor().add(k1, v1).add(k2, v2).add(k3, v3);
    }
}


---

üß© Exemple d‚Äôutilisation

Dataset<Row> df = ...; // dataset partitionn√© par dt et hour
List<String> partitionCols = Arrays.asList("dt", "hour");

// 1Ô∏è‚É£ Extraire les partitions distinctes
List<Row> rows = df.selectExpr(partitionCols.toArray(new String[0])).distinct().collectAsList();

// 2Ô∏è‚É£ Conversion automatique Row ‚Üí PartitionDescriptor
List<PartitionDescriptor> partitions = new ArrayList<>();
for (Row r : rows) {
    partitions.add(PartitionDescriptor.fromRow(r, partitionCols));
}

// 3Ô∏è‚É£ Exemple : affichage
for (PartitionDescriptor p : partitions) {
    System.out.println("Partition: " + p.toHivePath());
    // Ex: dt=2025-11-01/hour=13/
}

// 4Ô∏è‚É£ Lire uniquement les chemins n√©cessaires
String basePath = "abfss://container@storage.dfs.core.windows.net/out/my_table/";
List<String> paths = partitions.stream()
        .map(p -> basePath + p.toHivePath())
        .toList();

Dataset<Row> toLoad = spark.read().parquet(paths.toArray(new String[0]));

// 5Ô∏è‚É£ Charger dans ClickHouse (catalogue V2)
toLoad.writeTo("clickhouse.default.my_table").append();


---

üîç Avantages

Fonction	Description

fromRow(Row, List<String>)	Convertit instantan√©ment un Row Spark en objet partition
toHivePath()	G√©n√®re le chemin du dossier Parquet √† relire
toSqlPredicate()	Permet de filtrer dans Spark ou ClickHouse
asMap()	Compatible JSON / manifest / log
Serializable	Peut √™tre envoy√© √† l‚Äôex√©cuteur Spark ou stock√©



---

Souhaites-tu que je t‚Äôajoute une m√©thode fromDataset(Dataset<Row>, List<String>)
qui renvoie directement List<PartitionDescriptor> √† partir d‚Äôun Dataset<Row> (en une seule ligne, sans boucle manuelle) ?
