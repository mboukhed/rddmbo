Tu as lâ€™erreur Spark classique sous Java 17+ :

java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.DateTimeUtils 
cannot access class sun.util.calendar.ZoneInfo
because module java.base does not export sun.util.calendar

ğŸ”¥ Pourquoi Ã§a arrive ?

Parce que :

Tu exÃ©cutes Spark 3.x

Avec Java 17 ou Java 21

Spark utilise encore des classes internes JDK 8 (sun.util.calendar.ZoneInfo)

Mais Java 17/21 bloque lâ€™accÃ¨s Ã  ces classes internes (module system)


RÃ©sultat â†’ boom au runtime dans toutes les fonctions Spark qui appellent toJavaDate, DateTimeUtils, etc.

âœ”ï¸ Les 3 solutions possibles (ordre recommandÃ©)


---

âœ… Solution 1 â€” Lancer Spark avec les flags dâ€™ouverture des modules

Ajoute ceci Ã  ton spark-submit ou Ã  tes JVM options dans ta pipeline Jenkins / AKS :

--add-opens java.base/sun.util.calendar=ALL-UNNAMED
--add-opens java.base/java.util=ALL-UNNAMED

Ou en variable dâ€™environnement :

export JAVA_OPTS="$JAVA_OPTS --add-opens=java.base/sun.util.calendar=ALL-UNNAMED"
export JAVA_OPTS="$JAVA_OPTS --add-opens=java.base/java.util=ALL-UNNAMED"

Dans Spark :

spark.driver.extraJavaOptions   --add-opens=java.base/sun.util.calendar=ALL-UNNAMED
spark.executor.extraJavaOptions --add-opens=java.base/sun.util.calendar=ALL-UNNAMED

Câ€™est la vraie solution propre.


---

âœ… Solution 2 â€” Utiliser Java 11 pour exÃ©cuter Spark

Spark <= 3.5 nâ€™est pas totalement compatible Java 17.

Spark 4 sera full JDK17+.

Donc souvent la solution :

â¡ï¸ Mets Java 11 dans ton conteneur AKS pour exÃ©cuter les jobs Spark
â¡ï¸ Garde Java 17 pour compiler si tu veux


---

âœ… Solution 3 â€” Mettre Spark 3.5.1

Spark amÃ©liore la compat mais le bug nâ€™est pas totalement corrigÃ©.

Il faut quand mÃªme ajouter le --add-opens.


---

ğŸ‘ La bonne solution pour toi (en pipeline Jenkins)

Ajoute dans Maven Surefire plugin (pour les tests) :

<argLine>
 --add-opens java.base/sun.util.calendar=ALL-UNNAMED
 --add-opens java.base/java.util=ALL-UNNAMED
</argLine>

Pour exÃ©cuter Spark dans AKS :

Dans ton spark-submit :

--conf spark.driver.extraJavaOptions="--add-opens=java.base/sun.util.calendar=ALL-UNNAMED"
--conf spark.executor.extraJavaOptions="--add-opens=java.base/sun.util.calendar=ALL-UNNAMED"


---

ğŸ§¨ Pourquoi le test casse ?

Parce quâ€™un test Spark lit un champ date, et Spark dÃ©clenche DateTimeUtils.toJavaDate() â†’ qui tente dâ€™accÃ©der Ã  sun.util.calendar.ZoneInfo â†’ JDK17 dit non.


---

ğŸ“Œ RÃ©sumÃ© rapide

Option	RÃ©sultat

Ajouter --add-opens	âœ”ï¸ RÃ©sout immÃ©diatement le problÃ¨me
Revenir Java 11	âœ”ï¸ 100% stable mais pas compatible avec ton pipeline si tu compiles Java 17
Spark 3.5.1	ğŸ”§ AmÃ©liore mais ne corrige pas totalement



---

Si tu veux, je te gÃ©nÃ¨re le bloc complet Jenkinsfile + pom.xml + spark-submit avec les flags EXACTS pour ton environnement AKS.
