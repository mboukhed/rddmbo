Excellent üî• ‚Äî on va automatiser √ßa :
lire un Parquet ‚Üí r√©cup√©rer le sch√©ma Spark ‚Üí g√©n√©rer un CREATE TABLE ClickHouse complet,
avec gestion des Decimal(p,s), Date, Timestamp, String, etc.


---

‚úÖ 1Ô∏è‚É£ √âtape : lire le sch√©ma Parquet

Dataset<Row> df = spark.read()
    .option("mergeSchema", "true") // utile si r√©pertoire multi-fichiers
    .parquet("/chemin/vers/ton/fichier_ou_r√©pertoire.parquet");

StructType schema = df.schema();


---

‚úÖ 2Ô∏è‚É£ √âtape : convertir le sch√©ma Spark ‚Üí DDL ClickHouse

Voici une classe Java r√©utilisable üëá
Elle transforme le StructType en CREATE TABLE ‚Ä¶ ENGINE=ReplacingMergeTree(last_update)
et ajoute last_update + row_key automatiques (tu peux d√©sactiver si tu veux).

import org.apache.spark.sql.types.*;
import java.util.*;
import java.util.stream.Collectors;

public class ParquetToClickHouseDDL {

    public static String toClickHouseDDL(StructType schema,
                                         String database,
                                         String table,
                                         boolean addLastUpdate,
                                         boolean addRowKey) {

        List<String> cols = new ArrayList<>();

        for (StructField f : schema.fields()) {
            String colName = f.name();
            String chType = sparkToClickHouseType(f.dataType());
            cols.add("    `" + colName + "` " + chType);
        }

        if (addLastUpdate) {
            cols.add("    `last_update` DateTime DEFAULT now()");
        }

        if (addRowKey) {
            String allCols = Arrays.stream(schema.fieldNames())
                    .map(c -> "`" + c + "`")
                    .collect(Collectors.joining(", "));
            cols.add("    `row_key` UInt64 DEFAULT cityHash64(" + allCols + ")");
        }

        String colsBlock = String.join(",\n", cols);

        return String.format("""
            CREATE TABLE IF NOT EXISTS %s.%s (
            %s
            )
            ENGINE = ReplacingMergeTree(%s)
            ORDER BY %s;
            """,
            database,
            table,
            colsBlock,
            addLastUpdate ? "last_update" : "tuple()",
            addRowKey ? "row_key" : "tuple()");
    }

    private static String sparkToClickHouseType(DataType dt) {
        if (dt instanceof StringType) return "String";
        if (dt instanceof BooleanType) return "UInt8";
        if (dt instanceof ByteType) return "Int8";
        if (dt instanceof ShortType) return "Int16";
        if (dt instanceof IntegerType) return "Int32";
        if (dt instanceof LongType) return "Int64";
        if (dt instanceof FloatType) return "Float32";
        if (dt instanceof DoubleType) return "Float64";
        if (dt instanceof DateType) return "Date";
        if (dt instanceof TimestampType) return "DateTime";
        if (dt instanceof DecimalType d) {
            return "Decimal(" + d.precision() + "," + d.scale() + ")";
        }
        if (dt instanceof ArrayType arr) {
            return "Array(" + sparkToClickHouseType(arr.elementType()) + ")";
        }
        if (dt instanceof MapType map) {
            return "Map(" + sparkToClickHouseType(map.keyType()) + "," +
                   sparkToClickHouseType(map.valueType()) + ")";
        }
        return "String"; // fallback
    }
}


---

‚úÖ 3Ô∏è‚É£ √âtape : g√©n√©ration et affichage

String ddl = ParquetToClickHouseDDL.toClickHouseDDL(
    schema,
    "srv",                // base
    "avia_partition",     // nom de la table
    true,                 // ajouter last_update
    true                  // ajouter row_key (hash de toutes les colonnes)
);

System.out.println(ddl);

üü¢ Exemple de sortie :

CREATE TABLE IF NOT EXISTS srv.avia_partition (
    `integration_date` Date,
    `classification_level_1` String,
    `classification_level_2` String,
    `nb_passagers` Int32,
    `revenue` Decimal(18,4),
    `last_update` DateTime DEFAULT now(),
    `row_key` UInt64 DEFAULT cityHash64(integration_date, classification_level_1, classification_level_2, nb_passagers, revenue)
)
ENGINE = ReplacingMergeTree(last_update)
ORDER BY row_key;


---

‚úÖ 4Ô∏è‚É£ √âtape : ex√©cuter directement dans Spark (ClickHouse Catalog)

spark.sql(ddl);

‚Üí ta table ClickHouse est cr√©√©e automatiquement √† partir du sch√©ma Parquet.


---

üß† 5Ô∏è‚É£ R√©sum√© rapide

Besoin	M√©thode

R√©cup√©rer sch√©ma Parquet	spark.read().parquet(path).schema()
Convertir vers DDL ClickHouse	ParquetToClickHouseDDL.toClickHouseDDL(schema, db, table, ...)
G√©rer Decimal(p,s)	mapp√© vers Decimal(p,s) automatiquement
Cr√©er la table dans ClickHouse	spark.sql(ddl)
Recommand√©	ENGINE = ReplacingMergeTree(last_update) + row_key



---

Souhaites-tu que je te g√©n√®re la m√™me classe en Scala (plus concise, idiomatique) pour ton cluster Spark c√¥t√© job ?
